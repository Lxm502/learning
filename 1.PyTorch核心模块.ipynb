{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "e3f889d6-3927-45b0-8874-031fde870a2f",
   "metadata": {},
   "source": [
    "# 🔹 1. 核心模块\n",
    "\n",
    "1.1、torch：PyTorch 的核心张量运算模块，类似于 NumPy，但支持 GPU 加速。\n",
    "\n",
    "1.2、torch.nn：神经网络模块，提供常见的网络层（全连接、卷积、RNN 等）和损失函数。\n",
    "\n",
    "1.3、torch.optim：优化器模块，包含 SGD、Adam、RMSprop 等优化方法。\n",
    "\n",
    "1.4、torch.autograd：自动求导模块，支持反向传播和计算图。\n",
    "\n",
    "1.5、torch.utils.data：数据加载工具，提供 Dataset 和 DataLoader。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c305c0b6-45b3-47a1-8e90-5ef1bded9128",
   "metadata": {},
   "source": [
    "# 🐍 PyTorch `torch` 模块速查表\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 1. 张量创建（Tensor creation）\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| `torch.tensor(data)` | 从数据创建张量 |\n",
    "| `torch.empty(size)` | 未初始化的张量 |\n",
    "| `torch.zeros(size)` | 全 0 张量 |\n",
    "| `torch.ones(size)` | 全 1 张量 |\n",
    "| `torch.full(size, fill_value)` | 固定值填充，size：张量的形状,fill_value:填充值 |\n",
    "| `torch.arange(start, end, step)` | 范围序列 |\n",
    "| `torch.linspace(start, end, steps)` | 均匀分布序列 |\n",
    "| `torch.logspace(start, end, steps)` | 对数分布序列 |\n",
    "| `torch.eye(n)` | 单位矩阵（对角线元素为1，其余为0），n:指定输出矩阵的行数，m=None:指定输出矩阵的列数 |\n",
    "| `torch.rand(size)` | 均匀分布 [0,1) |\n",
    "| `torch.randn(size)` | 标准正态分布 |\n",
    "| `torch.randint(low, high, size)` | 均匀整数分布，low：随机整数下限(包含)，high：随机整数上限(不包含)，size：张量的形状 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 2. 张量操作（Tensor ops）\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| `torch.reshape(t, shape)` | 改变形状。t：要改变形状的输入张量。shape：目标形状，可以是tuple或list |\n",
    "| `torch.view(t, shape)` | 改变形状（共享内存） |\n",
    "| `torch.transpose(t, dim0, dim1)` | 交换维度(返回新的张量,共享内存)。t:要交换维度的输入张量。 dim0:要交换的第一个维度索引（从 0 开始）,dim1:要交换的第二个维度索引（从 0 开始）|\n",
    "| `torch.permute(t, dims)` | 任意维度换位(返回新的张量,共享内存)。dims: 新维度顺序的元组（必须包含所有维度的排列）|\n",
    "| `torch.cat([t1, t2], dim)` | 拼接（共享内存） t1、t2：要连接的张量(形状必须相同，除了dim维度。dim：沿着哪个维度拼接，默认dim=0)|\n",
    "| `torch.stack([t1, t2], dim)` | 堆叠（创建新维度） t1、t2：要堆叠的张量（形状必须完全相同）。dim：新维度的插入位置（默认dim=0）|\n",
    "| `torch.chunk(t, chunks, dim)` | 均匀分块(返回一个张量列表,共享内存)。chunks：分割的块数（不是每一块的大小）。 |\n",
    "| `torch.split(t, split_size, dim)` | 按大小分块(返回一个张量列表,共享内存)。split_size：每块的大小(int)或各块大小列表(list) |\n",
    "| `torch.tensor_split(t,indices_or_sections,dim)` | 不均匀分割。分割策略：(int:分割的块数，或, list of ints:指定分割位置的索引列表) |\n",
    "| `torch.squeeze(t)` | 去掉长度为 1 的维度 |\n",
    "| `torch.unsqueeze(t, dim)` | 增加维度(返回的是原始张量的视图（共享存储）):用于在指定位置插入长度为1的维度 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 3. 数学运算（Math ops）\n",
    "| 类别 | 方法 |\n",
    "|------|------|\n",
    "| 基本运算 |逐元素：加减乘除 `add`：+ / `sub`：- / `mul`：* / `div`：/ 。原地操作：函数_(),例如：add_()|\n",
    "| 幂与根 | `pow`：幂运算/ `sqrt`： 平方根。/ 原地操作：函数_(),例如：pow_(2)|\n",
    "| 指数对数 | `exp`： 自然指数函数​（以 e为底）/ `log`：自然对数​（以 e为底） / `log10` / `log2` |\n",
    "| 三角函数 | `sin` / `cos` / `tan` / `atan2` |\n",
    "| 统计 | `mean`： 平均值/ `median`：中位数 / `mode`：众数 / `std`： 标准差/ `var`： 方差|\n",
    "| 归约 | `sum`： 求和/ `prod`： 连乘/ `cumsum`： 累积和/ `cumprod`： 累积乘积|\n",
    "| 矩阵运算 | `mm`：严格的二维矩阵乘法 / `matmul`：广义矩阵乘法​（支持广播和高维张量） / `bmm`：批量矩阵乘法(3D张量) |\n",
    "| 线性代数 | `inverse`： 计算方阵的逆矩阵/ `det`：计算方阵的行列式 / `svd`：计算矩阵的奇异值分解 / `linalg.eig`：计算方阵的特征值与特征向量 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 4. 比较 & 逻辑\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| `eq` / `ne` | 相等 / 不等 / 举例：torch.eq(x,y) 等价写法：x == y|\n",
    "| `gt` / `ge` | 大于 / 大于等于 /  举例：torch.gt(x,y) 等价写法：x > y|\n",
    "| `lt` / `le` | 小于 / 小于等于 /  举例：torch.le(x,y) 等价写法：x <= y |\n",
    "| `all` / `any` |all：判断张量中所有元素是否均为 True​（或非零/非空）/any:判断张量中是否存在至少一个 True​（或非零/非空） |\n",
    "| `isfinite`：：检测张量中的有限数值​（非无穷且非NaN） / `isnan` / `isinf` | 判断数值状态 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 5. 随机数\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| `torch.manual_seed(seed)` | 固定随机种子 |\n",
    "| `rand` / `randn` / `randint` | 常用分布:均匀分布[0,1)/标准准态分布（0，1）/离散均匀分布[最小值，最大值） |\n",
    "| `bernoulli` | 伯努利分布 |\n",
    "| `normal(mean, std)` | 正态分布 |\n",
    "| `multinomial` | 多项分布 |\n",
    "| `randperm(n)` | 随机排列 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 6. 自动求导（Autograd）\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| `torch.autograd.backward(tensors, grad_tensors)` | 反向传播 |\n",
    "| `torch.autograd.grad(outputs, inputs)` | 手动求导 |\n",
    "| `torch.set_grad_enabled(mode)` | 控制梯度开关 |\n",
    "| `torch.no_grad()` | 禁用梯度（推理时用） |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 7. 神经网络 & 优化\n",
    "| 模块 | 说明 |\n",
    "|------|------|\n",
    "| `torch.nn` | 神经网络层、损失函数 |\n",
    "| `torch.nn.functional` | 函数式 API (如 `F.relu`) |\n",
    "| `torch.optim` | 优化器 (SGD, Adam...) |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 8. 数据加载\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| `torch.utils.data.Dataset` | 数据集抽象类 |\n",
    "| `torch.utils.data.DataLoader` | 数据批加载 |\n",
    "| `torchvision.datasets` | 常用图像数据集 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 9. 设备与并行\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| `torch.device(\"cpu\"/\"cuda\")` | 指定设备 |\n",
    "| `torch.cuda.is_available()` | CUDA 是否可用 |\n",
    "| `torch.cuda.manual_seed(seed)` | CUDA 随机种子 |\n",
    "| `torch.cuda.empty_cache()` | 清理显存 |\n",
    "| `torch.nn.DataParallel` | 多 GPU |\n",
    "| `torch.distributed` | 分布式训练 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 10. 保存与加载\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| `torch.save(obj, path)` | 保存模型/张量 |\n",
    "| `torch.load(path)` | 加载 |\n",
    "| `torch.jit` | TorchScript 序列化 |\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 11. 其他常用\n",
    "| 方法 | 说明 |\n",
    "|------|------|\n",
    "| `torch.clone(t)` | 克隆张量 |\n",
    "| `torch.detach()` | 去掉梯度信息 |\n",
    "| `torch.numel(t)` | 元素个数 |\n",
    "| `torch.size(t)` / `t.shape` | 形状 |\n",
    "| `torch.topk(t, k)` | 取前 k 大值 |\n",
    "| `torch.argmax` / `torch.argmin` | 最大/最小索引 |\n",
    "| `torch.where(cond, x, y)` | 条件选择 |\n",
    "| `torch.nonzero(t)` | 非零索引 |\n",
    "\n",
    "---\n",
    "### ✅ 总结：**torch = 张量创建 + 操作 + 计算 + 自动求导 + 神经网络 + 优化 + 数据加载 + 并行 + 序列化**。\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3092ab13-e325-4458-9d87-149cc99a17e0",
   "metadata": {},
   "source": [
    "# 1.1、torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e485fd58-c695-4a92-a923-b7e8c8d9676e",
   "metadata": {},
   "source": [
    "# 🧭 torch.tensor() \n",
    "\n",
    "## 📌1) 基本签名与作用\n",
    "###  torch.tensor(data, *, dtype=None, device=None, requires_grad=False, pin_memory=False)\n",
    "\n",
    "✅作用：把 Python 数据（标量、list/嵌套list、tuple、NumPy 数组、其他张量等）转换成一个新的 torch.Tensor。\n",
    "\n",
    "核心特性：默认会拷贝数据（与 torch.as_tensor 不同），因此不会与原始输入共享内存。\n",
    "\n",
    "---\n",
    "\n",
    "## 📌2) 主要参数\n",
    "\n",
    "| 参数              | 说明                                                       |\n",
    "| --------------- | -------------------------------------------------------- |\n",
    "| `data`          | Python 数据（标量、list/嵌套 list、tuple、NumPy、Tensor）            |\n",
    "| `dtype`         | 数据类型（默认：整数→`torch.int64`，浮点→`torch.get_default_dtype()`） |\n",
    "| `device`        | 存放设备，如 `'cpu'`、`'cuda:0'`                                |\n",
    "| `requires_grad` | 是否参与自动求导（仅浮点/复数有效）                                       |\n",
    "| `pin_memory`    | 是否固定在页锁内存，加速 CPU→GPU 拷贝（仅对 CPU 张量有效）                     |\n",
    "\n",
    "---\n",
    "\n",
    "## 📌3) dtype 推断与默认行为\n",
    "\n",
    "全是整数 → torch.int64\n",
    "\n",
    "含浮点 → 默认浮点类型（torch.float32，可用 torch.get_default_dtype() 查看/设置）\n",
    "\n",
    "布尔 → torch.bool\n",
    "\n",
    "混合类型 → 类型提升，如 [1, 2.5] → float32\n",
    "\n",
    "---\n",
    "\n",
    "## 📌4) 形状与维度\n",
    "\n",
    "标量 → 0 维张量（shape == ()）。\n",
    "\n",
    "扁平 list → 1 维；嵌套 list → 多维。必须是规则矩阵（每层列表长度一致），否则报错。\n",
    "\n",
    "---\n",
    "\n",
    "## 📌5) 拷贝/共享内存：与其他构造器的区别\n",
    "\n",
    "这块是很多人最容易踩坑的地方：\n",
    "| 构造方式                   | 是否复制           | 与输入共享内存                              | 是否保留梯度图（当输入是 Tensor）                            |\n",
    "| ---------------------- | -------------- | ------------------------------------ | ----------------------------------------------- |\n",
    "| `torch.tensor(x)`      | **总是尽量复制**     | 否                                    | **不保留**（会断开计算图；且默认 `requires_grad=False`）       |\n",
    "| `torch.as_tensor(x)`   | 尽量**不复制**      | 尽可能共享（尤其是来自 `np.ndarray` 或已有 Tensor） | 是（若 `x` 本身是 Tensor，就直接返回它，保留图与 `requires_grad`） |\n",
    "| `torch.from_numpy(nd)` | **不复制**（满足条件时） | 是（与 `nd` 共享内存）                       | 不涉及计算图；对 `nd` 的写会反映到张量，反之亦然                     |\n",
    "\n",
    "| 方法                          | 是否拷贝  | 是否共享内存 | 梯度计算图             |\n",
    "| --------------------------- | ----- | ------ | ----------------- |\n",
    "| `torch.tensor(x)`           | ✅ 拷贝  | ❌ 不共享  | ❌ 不保留（断开）         |\n",
    "| `torch.as_tensor(x)`        | 尽量不拷贝 | 尽量共享   | ✅ 保留              |\n",
    "| `torch.from_numpy(ndarray)` | ❌ 不拷贝 | ✅ 共享   | 无计算图；与 NumPy 共享数据 |\n",
    "| `x.clone()`                 | ✅ 拷贝  | ❌ 不共享  | ✅ 保留梯度属性          |\n",
    "| `x.detach()`                | ❌ 不拷贝 | ✅ 共享   | ❌ 切断梯度            |\n",
    "\n",
    "\n",
    "### 示例要点"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac50a6fe-315c-4c92-9d10-1e20416176d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "\n",
    "a = np.array([1, 2, 3], dtype=np.int64)  # a 指向一块内存：[1, 2, 3]\n",
    "\n",
    "t1 = torch.tensor(a)        # ✅ 复制数据（新建内存）\n",
    "t2 = torch.as_tensor(a)     # 🔗 共享内存（不复制）\n",
    "t3 = torch.from_numpy(a)    # 🔗 共享内存（不复制）\n",
    "\n",
    "a[0] = 999  # 修改原始 NumPy 数组的第一个元素\n",
    "\n",
    "结果：\n",
    "t1[0] 仍然是 1 → 因为 t1 是复制的，修改 a 不影响它。\n",
    "t2[0] 变成 999\n",
    "t3[0] 变成 999\n",
    "👉 因为 t2 和 t3 共享 a 的内存，所以 a[0] = 999 修改了那块内存，t2 和 t3 读取时自然看到变化。\n",
    "\n",
    "\n",
    "\n",
    "# 1) 基本创建\n",
    "a = torch.tensor([1, 2, 3])                    # int64\n",
    "b = torch.tensor([1.0, 2, 3])                  # float32\n",
    "c = torch.tensor([[1, 2], [3, 4]], dtype=torch.float64)\n",
    "\n",
    "# 2) 指定设备 & 梯度\n",
    "g = torch.tensor([0.1, 0.2], device='cuda', requires_grad=True)\n",
    "\n",
    "# 3) NumPy 互操作\n",
    "import numpy as np\n",
    "arr = np.arange(6).reshape(2,3)\n",
    "t1 = torch.tensor(arr)         # 拷贝\n",
    "t2 = torch.from_numpy(arr)     # 共享内存\n",
    "arr[0,0] = 999                 # t2[0,0] 跟着变，t1 不变\n",
    "\n",
    "# 4) 标量 / 0维张量\n",
    "s = torch.tensor(3.14)         # shape: ()\n",
    "float(s)                       # -> 3.14 (Python float)\n",
    "\n",
    "# 5) 修改默认浮点 dtype\n",
    "torch.set_default_dtype(torch.float64)\n",
    "u = torch.tensor([1.0, 2.0])   # float64\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "786679ff-bf1b-4bad-9066-baae4be2a5d4",
   "metadata": {},
   "source": [
    "### 对已有 Tensor 的行为"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "255bd1bc-6a20-48d0-8870-bb31944626e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y = torch.tensor(x)         # 新张量，拷贝数据，计算图被断开，默认 requires_grad=False\n",
    "z = torch.as_tensor(x)      # 返回 x 本身（或视图），保留计算图与 requires_grad=True\n",
    "w = x.clone()               # 拷贝数据，保留 dtype/device，仍在同一计算图上（对 autograd 通常更安全）\n",
    "d = x.detach()              # 不拷贝数据的“视图”，但切断梯度（不再参与反向）\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e51c501c-63ed-44de-a6d6-1353922f6445",
   "metadata": {},
   "source": [
    "### 👉小结： \n",
    "需要拷贝且不保留梯度 → torch.tensor(x)；\\\n",
    "能不拷贝就不拷贝 → torch.as_tensor(x) / torch.from_numpy(nd)；\\\n",
    "拷贝但保留梯度属性 → x.clone()（或 x.clone().detach() 视场景）。\n",
    "\n",
    "---\n",
    "\n",
    "## 📌6) device 与 pin_memory\n",
    "\n",
    "直接指定设备：\n",
    "\n",
    "t = torch.tensor([1,2,3], device='cuda')    # 直接在 GPU 上创建\n",
    "\n",
    "若无可用 GPU 会报错。\n",
    "\n",
    "pinned memory（仅 CPU 张量）：\n",
    "\n",
    "t = torch.tensor([1,2,3], pin_memory=True)  # 之后 t.to('cuda') 更快\n",
    "\n",
    "pin_memory=True 不能用于 device='cuda' 的创建。\n",
    "\n",
    "---\n",
    "\n",
    "## 📌7) requires_grad 与 autograd\n",
    "\n",
    "只有浮点或复数张量才能 requires_grad=True。整型/布尔会报错或被忽略。\n",
    "\n",
    "若你打算把这个张量作为可训练参数或参与可微运算，设置：\n",
    "\n",
    "w = torch.tensor([0.1, 0.2], dtype=torch.float32, requires_grad=True)\n",
    "\n",
    "用 torch.tensor(tensor_input) 会把 requires_grad 复位为 False 且断开图，很多人因此梯度为 None。保持图请用 as_tensor 或 clone()。\n",
    "\n",
    "---\n",
    "\n",
    "## 📌8) 常用示例"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "642a6beb-dab2-42f8-8c37-e843c9a12b9a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1) 基本创建（自动推断）\n",
    "a = torch.tensor([1, 2, 3])                          # int64\n",
    "b = torch.tensor([1.0, 2, 3])                        # float32（默认浮点）\n",
    "c = torch.tensor([[1, 2], [3, 4]], dtype=torch.float64)\n",
    "\n",
    "# 2) 指定设备 & 梯度\n",
    "g = torch.tensor([0.1, 0.2], device='cuda', requires_grad=True)\n",
    "\n",
    "# 3) 从 NumPy（是否共享内存对比）\n",
    "import numpy as np\n",
    "arr = np.arange(6).reshape(2,3)\n",
    "t_copy = torch.tensor(arr)        # 拷贝\n",
    "t_share = torch.from_numpy(arr)   # 共享\n",
    "arr[0,0] = 999                    # 仅 t_share 受影响\n",
    "\n",
    "# 4) 标量/0维张量\n",
    "s = torch.tensor(3.14)            # shape: ()\n",
    "float(s)                          # 3.14（Python 标量）\n",
    "\n",
    "# 5) 修改默认浮点 dtype（谨慎）\n",
    "torch.set_default_dtype(torch.float64)\n",
    "u = torch.tensor([1.0, 2.0])      # 现在是 float64\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e5c9e3-0b07-4ff7-9030-504b2c829ee3",
   "metadata": {},
   "source": [
    "## 9) 常见坑（踩雷清单）\n",
    "\n",
    "无意中断开计算图：\n",
    "用 torch.tensor(existing_tensor) 会断开图且 requires_grad=False。想保留 → existing_tensor.clone() 或直接用 as_tensor(existing_tensor)。\n",
    "\n",
    "误以为一定共享内存：\n",
    "torch.tensor(np_array) 是拷贝；要共享用 torch.from_numpy() 或 as_tensor(np_array)。\n",
    "\n",
    "在 GPU 上设 pinned memory：\n",
    "pin_memory=True 只对 CPU 张量有效；GPU 上无意义且可能报错。\n",
    "\n",
    "整数/布尔设 requires_grad=True：\n",
    "梯度只对浮点/复数有效，整型/布尔不行。\n",
    "\n",
    "不规则嵌套 list：\n",
    "形状必须规则，否则报错。\n",
    "\n",
    "默认 dtype 误判：\n",
    "整数默认 int64，浮点默认 get_default_dtype()；跨项目时若有人改过默认浮点为 float64，会导致显存翻倍或数值差异。\n",
    "\n",
    "---\n",
    "\n",
    "## 10) 什么时候不用 torch.tensor()？\n",
    "\n",
    "需要就地初始化（不从现有数据来）→ 用工厂函数更快：\n",
    "\n",
    "torch.zeros/ones/empty/full/rand/randn/arange/linspace 等，并能直接指定 dtype/device。\n",
    "\n",
    "想与 NumPy 高效互操作（共享内存）→ torch.from_numpy() / torch.as_tensor()。\n",
    "\n",
    "想保留梯度信息复制一个已有张量 → x.clone()（必要时再 .detach()）。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e56a0b3-22c8-4724-b091-c2031609cdde",
   "metadata": {},
   "source": [
    "# 🚨 常见坑清单\n",
    "\n",
    "### 1、断开计算图\n",
    "\n",
    "y = torch.tensor(x)   # 会拷贝数据 & requires_grad=False\n",
    "\n",
    "→ 想保留梯度：用 x.clone() 或 torch.as_tensor(x)。\n",
    "\n",
    "### 2、误以为总是共享内存\n",
    "\n",
    "torch.tensor(np_array) 是 拷贝，要共享请用 torch.from_numpy() 或 torch.as_tensor()。\n",
    "\n",
    "### 3、在 GPU 上用 pin_memory\n",
    "pin_memory=True 仅对 CPU 张量 有效；GPU 张量会报错。\n",
    "\n",
    "### 4、整数/布尔张量设 requires_grad=True\n",
    "只有浮点/复数支持梯度，整型/布尔不行。\n",
    "\n",
    "### 5、不规则嵌套 list\n",
    "形状必须规则，否则报错。\n",
    "\n",
    "### 6、默认 dtype 被改动\n",
    "项目中若有人 torch.set_default_dtype(torch.float64)，会导致显存翻倍或数值不一致。\n",
    "\n",
    "## ✅ 建议：\n",
    "\n",
    "数据初始化 → 用 torch.zeros/ones/rand/... 更高效\n",
    "\n",
    "NumPy 转 Tensor 且需要共享 → torch.from_numpy()\n",
    "\n",
    "需要保留梯度 → x.clone()（或 x.clone().detach()）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53d835f3-9bbc-4564-bb6f-6a888e51b31f",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a671074-4549-4b16-a004-694078c17c8a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9107d804-f7ae-4267-aa2d-9b1f725e270d",
   "metadata": {},
   "source": [
    "# 🧭torch.autograd\n",
    "它是 PyTorch 的 自动微分核心模块。理解了 autograd，基本就能掌握 PyTorch 的“自动梯度”原理。\n",
    "\n",
    "## 1. 概念\n",
    "\n",
    "autograd = 自动求导引擎\n",
    "\n",
    "本质：在 前向计算 时，PyTorch 会 动态构建一张计算图 (DAG)，记录张量之间的运算关系；在 反向传播 时，autograd 会从输出往输入回溯，利用 链式法则 自动计算梯度。\n",
    "\n",
    "适用于 标量 loss 对 模型参数 的梯度计算，是训练神经网络的关键。\n",
    "\n",
    "## 2. autograd 的核心属性\n",
    "### 👉(1) requires_grad\n",
    "\n",
    "控制一个张量是否需要梯度。\n",
    "\n",
    "默认 False，需要手动打开：x = torch.ones(3, requires_grad=True)\n",
    "\n",
    "只有 浮点 / 复数 张量能设置 requires_grad=True。\n",
    "\n",
    "### 👉(2) .grad\n",
    "\n",
    "保存 梯度结果（即 ∂loss/∂x）。\n",
    "\n",
    "只对 叶子节点（leaf tensors） 有意义：\n",
    "\n",
    "叶子节点 = 直接由用户创建的、requires_grad=True 的张量。\n",
    "\n",
    "中间节点的 .grad 默认不会存储（节省内存）。\n",
    "\n",
    "### 👉(3) .grad_fn\n",
    "\n",
    "每个 非叶子张量 都有一个 .grad_fn 属性，记录它是由哪种函数（操作）创建的。\n",
    "\n",
    "例如：\n",
    "x = torch.ones(2, requires_grad=True)  \\\n",
    "y = x + 2  \\\n",
    "print(y.grad_fn)   \n",
    "\n",
    "### 👉(4) 计算图 (Dynamic Computation Graph)\n",
    "\n",
    "动态图：每次前向传播时都会重新构建，不像 TensorFlow 1.x 那样要静态编图。\n",
    "\n",
    "在 backward() 时，这个图会被释放（除非设置 retain_graph=True）。\n",
    "\n",
    "## 3. autograd 的核心接口\n",
    "| 函数                                                                                           | 作用                                 |\n",
    "| -------------------------------------------------------------------------------------------- | ---------------------------------- |\n",
    "| `torch.autograd.backward(tensors, grad_tensors, ...)`                                        | 核心反向传播接口（大部分时候通过 `.backward()` 调用） |\n",
    "| `tensor.backward(grad=None, ...)`                                                            | 常用的简化接口                            |\n",
    "| `torch.autograd.grad(outputs, inputs, grad_outputs, create_graph=False, retain_graph=False)` | 直接返回梯度，而不是累加到 `.grad`，常用于高阶梯度      |\n",
    "| `torch.autograd.set_detect_anomaly(True)`                                                    | 开启异常检测，定位梯度爆炸/NaN 的来源              |\n",
    "| `torch.no_grad()`                                                                            | 上下文管理器，临时禁用梯度计算（推理模式）              |\n",
    "| `torch.enable_grad()`                                                                        | 与 `no_grad` 对应，重新启用梯度              |\n",
    "| `torch.inference_mode()`                                                                     | 更高效的 **推理模式**，彻底禁用 autograd 记录     |\n",
    "\n",
    "## 4. 运行机制（例子）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f79b2e38-a22e-49a1-b69e-9c20a3761ae3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x ** 2 + 3 * x\n",
    "\n",
    "# 反向传播\n",
    "y.backward()\n",
    "print(x.grad)   # dy/dx = 2*x + 3 = 7\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98e0a99b-9936-46f4-b28e-5bda76cf4819",
   "metadata": {},
   "outputs": [],
   "source": [
    "流程：\n",
    "\n",
    "1、前向：构建图 y = x^2 + 3x，保存操作记录。\n",
    "\n",
    "2、调用 y.backward()：反向遍历计算图。\n",
    "\n",
    "3、根据链式法则求导：dy/dx = 2x + 3 = 7\n",
    "\n",
    "4、结果存到 x.grad。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d7c56603-06b0-4177-ba90-695912e4a8be",
   "metadata": {},
   "source": [
    "## 5. 高阶用法\n",
    "### (1) 高阶梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20f8d7b8-a7a1-4a4e-8291-0023f86127d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**3\n",
    "\n",
    "# 一阶导数\n",
    "grad1 = torch.autograd.grad(y, x, create_graph=True)[0]\n",
    "\n",
    "# 二阶导数\n",
    "grad2 = torch.autograd.grad(grad1, x)[0]\n",
    "print(grad1, grad2)   # 12, 6\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f9baec5e-45bf-473d-80f1-cc63dd15f940",
   "metadata": {},
   "source": [
    "### (2) 禁用梯度（推理/评估时）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2bcbc48b-9fe4-4b73-a241-8baeb977bf34",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.no_grad():\n",
    "    y = model(x)  # 不会追踪梯度\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f363844b-7d77-4af0-97d2-43a8bd1d6f67",
   "metadata": {},
   "source": [
    "### (3) 推理模式（PyTorch 1.9+）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8b8f28e-eb3b-4054-9d77-0e43192d7cfd",
   "metadata": {},
   "outputs": [],
   "source": [
    "with torch.inference_mode():\n",
    "    y = model(x)  # 比 no_grad 更快更省内存\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8e0eb67f-f038-4748-952f-44784d906bde",
   "metadata": {},
   "source": [
    "### (4) 只对部分输入求梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcfc4797-c3cc-4161-adca-d7a3436b94fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "c = a * b\n",
    "\n",
    "grad_a = torch.autograd.grad(c, a)[0]  # ∂c/∂a = b\n",
    "print(grad_a)  # 3\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a242247b-4def-4301-b014-d2952f4e8fd1",
   "metadata": {},
   "source": [
    "## 6. 常见坑 ⚠️\n",
    "1.梯度会累加\\\n",
    "每次 backward() 会把结果累加到 .grad。训练循环里要 optimizer.zero_grad()。\n",
    "\n",
    "2.非标量输出必须传梯度 \\\n",
    "y = model(x)   # y 不是标量 \\\n",
    "y.backward()   # ❌ 报错\\\n",
    "y.backward(torch.ones_like(y))   # ✅\n",
    "\n",
    "\n",
    "3.in-place 操作可能破坏计算图\\\n",
    "对 requires_grad=True 的张量做原地修改可能导致梯度错误。\\\n",
    "❌ x += 1\\\n",
    "✅ x = x + 1\n",
    "\n",
    "4.整数 / 布尔不能 requires_grad\\\n",
    "只有浮点 / 复数才行。\n",
    "\n",
    "5.中间结果默认不保存 .grad\\\n",
    "如果想要中间节点的梯度，要用 torch.autograd.grad()。\n",
    "\n",
    "## 7. 学习速查表\n",
    "# torch.autograd 速查表\n",
    "\n",
    "## 核心属性\n",
    "- `requires_grad` : 是否追踪梯度\n",
    "- `.grad`         : 存放梯度结果（仅叶子张量）\n",
    "- `.grad_fn`      : 创建该张量的 Function\n",
    "- 计算图 (DAG)   : 前向时动态构建，反向传播时释放\n",
    "\n",
    "## 常用接口\n",
    "| 函数 | 功能 |\n",
    "|------|------|\n",
    "| `tensor.backward()` | 反向传播（标量输出可省略 grad） |\n",
    "| `torch.autograd.backward()` | 通用接口（支持多个输出） |\n",
    "| `torch.autograd.grad()` | 返回梯度，不存到 `.grad` |\n",
    "| `torch.no_grad()` | 临时禁用梯度（推理用） |\n",
    "| `torch.inference_mode()` | 更快的推理模式 |\n",
    "| `torch.autograd.set_detect_anomaly(True)` | 开启异常检测 |\n",
    "\n",
    "## 常见坑 ⚠️\n",
    "1. `.grad` 会累加 → 训练时要 `zero_grad()`\n",
    "2. 非标量 `.backward()` 需传 `grad`\n",
    "3. inplace 操作可能破坏计算图\n",
    "4. 只有浮点/复数张量能 `requires_grad=True`\n",
    "5. 中间节点默认不保存 `.grad`（用 `autograd.grad()` 获取）\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fdef8ed1-5658-4e09-9ffa-d13c7054c819",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbe1bd42-24f1-4b01-b4ef-cb87cab02e62",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "848d0bcf-e905-431f-a90e-09238b2e2ba2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "aba6aafb-bae3-49cc-ab3f-5038a9c8a8e4",
   "metadata": {},
   "source": [
    "# torch.autograd.backward() \n",
    "是 PyTorch 自动微分引擎 autograd 的核心接口之一，很多人只知道 loss.backward()，但其实它背后调用的就是 torch.autograd.backward()。\n",
    "\n",
    "## 1. 定义"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f273ca37-1dc9-4eb7-8bea-e7dccfe694da",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.autograd.backward(\n",
    "    tensors,\n",
    "    grad_tensors=None,\n",
    "    retain_graph=None,\n",
    "    create_graph=False,\n",
    "    grad_variables=None,   # 旧版本别名\n",
    "    inputs=None\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54bd40e0-3dc6-4dc4-a156-567217052d69",
   "metadata": {},
   "source": [
    "它的作用是：\n",
    "给定一个或多个张量 tensors（通常是 loss 或输出），计算它们对叶子节点（如模型参数）的梯度，并把结果累加到 .grad 属性里。\n",
    "\n",
    "等价于在张量上调用 .backward()，但更灵活，可以同时对多个目标做反向传播。\n",
    "\n",
    "## 2. 参数详解\n",
    "| 参数                | 类型                     | 说明                                                              |\n",
    "| ----------------- | ---------------------- | --------------------------------------------------------------- |\n",
    "| **tensors**       | Tensor 或 list\\[Tensor] | 需要反向传播的“终点”（通常是 loss，或者是你指定的函数输出）                               |\n",
    "| **grad\\_tensors** | Tensor 或 list\\[Tensor] | 与 `tensors` 形状相同，作为每个输出的“外部梯度” (∂L/∂y)。若标量 loss，可省略；若非标量，必须指定。  |\n",
    "| **retain\\_graph** | bool                   | 是否保留计算图。默认 `False`，一次反向传播后释放；设为 `True` 可多次 backward（如 RNN 训练时）。 |\n",
    "| **create\\_graph** | bool                   | 是否创建更高阶的计算图（允许对梯度再次求导，即高阶导数）。默认 `False`。                        |\n",
    "| **inputs**        | list\\[Tensor]          | 指定只对哪些 Tensor 求梯度。若为 `None`，默认对所有叶子节点计算。                        |\n",
    "\n",
    "## 3. 工作机制\n",
    "\n",
    "autograd 会把 前向计算过程中构建的计算图 反向遍历。\n",
    "\n",
    "从 tensors 出发，把 grad_tensors 作为初始梯度（链式法则的起点），一路传播到需要梯度的叶子节点。\n",
    "\n",
    "最终结果写入各张量的 .grad 属性（注意是 累加 而不是覆盖）。\n",
    "\n",
    "## 4. 常见用法\n",
    "### (1) 最基本：标量 loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca3952a6-8f90-4095-b7e1-71a0685de0e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "x = torch.ones(2, 2, requires_grad=True)\n",
    "y = (x * 3).sum()   # 标量\n",
    "torch.autograd.backward(y)\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "270f3e74-8ecf-4493-ad17-9e850cc51c42",
   "metadata": {},
   "source": [
    "#### 相当于 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cb466a9b-458a-40ed-b9d4-2ec3ecafc01d",
   "metadata": {},
   "outputs": [],
   "source": [
    "y.backward()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d35756f-43fc-470e-878e-caccca0f6a23",
   "metadata": {},
   "source": [
    "### (2) 非标量张量\n",
    "\n",
    "必须指定 grad_tensors，否则 PyTorch 不知道“往下传播什么梯度”。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47be2790-bd8f-4f25-a1a3-3bc86900d76e",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([[1., 2.], [3., 4.]], requires_grad=True)\n",
    "y = x * 2           # y.shape = (2,2)，不是标量\n",
    "\n",
    "grad_outputs = torch.ones_like(y)   # 每个元素权重=1\n",
    "torch.autograd.backward(y, grad_tensors=grad_outputs)\n",
    "print(x.grad)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f27af55-ed26-4dd7-8437-c6f289ee3760",
   "metadata": {},
   "source": [
    "### (3) 多个目标一起反传"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7f721b1d-f66e-4563-bf28-a9489f480d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor([1., 2., 3.], requires_grad=True)\n",
    "y1 = (x**2).sum()     # ∑ x^2\n",
    "y2 = (x**3).sum()     # ∑ x^3\n",
    "\n",
    "torch.autograd.backward([y1, y2], grad_tensors=[torch.tensor(1.), torch.tensor(0.1)])\n",
    "print(x.grad)\n",
    "\n",
    "👉 意思是：总梯度 = 1·∂y1/∂x + 0.1·∂y2/∂x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29e3a33c-98d8-4e8b-863d-1eba8b9ba893",
   "metadata": {},
   "source": [
    "### (4) 高阶导数"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d121b0bd-39fe-4ba8-abb7-5cb827f25b2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.tensor(2.0, requires_grad=True)\n",
    "y = x**3\n",
    "\n",
    "# 一阶梯度\n",
    "torch.autograd.backward(y, create_graph=True)\n",
    "dy_dx = x.grad        # 3*x^2 = 12\n",
    "print(dy_dx)\n",
    "\n",
    "# 二阶梯度\n",
    "dy_dx.backward()\n",
    "print(x.grad)         # 注意 grad 会累加，所以结果是 12 + 12 = 24\n",
    "\n",
    "👉要避免累加错误，记得 x.grad.zero_()。"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e54cb7e8-8b57-496b-b995-8b97dd6de4e2",
   "metadata": {},
   "source": [
    "### (5) 限制计算某些 inputs 的梯度"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab83f759-a2b8-4b9e-8d7a-bf94617e50b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "a = torch.tensor(2.0, requires_grad=True)\n",
    "b = torch.tensor(3.0, requires_grad=True)\n",
    "c = a * b\n",
    "\n",
    "torch.autograd.backward(c, inputs=[a])   # 只对 a 求梯度\n",
    "print(a.grad)   # 3\n",
    "print(b.grad)   # None\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c693f3e-0990-4a2c-8c31-6e85a2f91326",
   "metadata": {},
   "source": [
    "## 5. 常见坑点 ⚠️\n",
    "\n",
    "1.非标量必须传 grad_tensors\n",
    "\n",
    "y = model(x)  # y.shape=(batch, num_classes)\n",
    "y.backward()  # ❌ 报错\n",
    "\n",
    "✅ 做法：\n",
    "\n",
    "y.backward(torch.ones_like(y))\n",
    "\n",
    "2.梯度会累加：\n",
    "每次调用 backward()，结果会累加到 .grad，不是覆盖。训练循环里要 optimizer.zero_grad()。\n",
    "\n",
    "3.retain_graph ：\n",
    "如果你在同一个计算图上多次调用 backward()，必须 retain_graph=True，否则第二次会报错。\n",
    "\n",
    "4.整数/布尔张量不能反传：\n",
    "只有浮点/复数张量能 requires_grad=True，否则报错。\n",
    "\n",
    "5.in-place 操作可能破坏计算图：\n",
    "在 requires_grad=True 的 Tensor 上做 inplace 修改，可能导致梯度计算错误。\n",
    "\n",
    "## 6. 总结\n",
    "\n",
    "loss.backward() = torch.autograd.backward(loss)（更常用）。\n",
    "\n",
    "非标量输出要传 grad_tensors。\n",
    "\n",
    "多个目标 → backward([y1, y2], grad_tensors=[g1, g2])。\n",
    "\n",
    "高阶导数 → create_graph=True。\n",
    "\n",
    "梯度累加 → 记得 zero_grad()。\n",
    "\n",
    "inputs 参数可以只对部分张量求梯度。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "566a33c9-e454-4d9d-a0d6-5e8659b5258c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepseek_env]",
   "language": "python",
   "name": "conda-env-deepseek_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
