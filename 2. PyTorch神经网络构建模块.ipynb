{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "44a8bcfb-b2d1-46cc-8ba2-6963ee3fba63",
   "metadata": {},
   "source": [
    "# 🔹 2. 神经网络构建模块\n",
    "\n",
    "## 2.1、层 (Layers)\n",
    "\n",
    "nn.Linear：全连接层\n",
    "\n",
    "nn.Conv1d / nn.Conv2d / nn.Conv3d：卷积层\n",
    "\n",
    "nn.RNN / nn.LSTM / nn.GRU：循环神经网络层\n",
    "\n",
    "nn.Embedding：嵌入层（常用于 NLP）\n",
    "\n",
    "nn.BatchNorm1d / nn.BatchNorm2d：批归一化层\n",
    "\n",
    "nn.Dropout：Dropout 层\n",
    "\n",
    "## 2.2、激活函数 (Activations)\n",
    "\n",
    "nn.ReLU, nn.Sigmoid, nn.Tanh, nn.Softmax, nn.LeakyReLU, nn.GELU 等\n",
    "\n",
    "## 2.3、损失函数 (Losses)\n",
    "\n",
    "nn.CrossEntropyLoss：分类任务\n",
    "\n",
    "nn.MSELoss：回归任务\n",
    "\n",
    "nn.BCELoss / nn.BCEWithLogitsLoss：二分类任务\n",
    "\n",
    "nn.HingeEmbeddingLoss, nn.MarginRankingLoss 等特殊损失"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f3318e3-c6e2-43cd-9e63-f60028abec99",
   "metadata": {},
   "source": [
    "# 🧩 torch.nn 速查表\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 容器\n",
    "- `nn.Module` → 基类\n",
    "- `nn.Sequential` → 顺序容器（按顺序堆叠层）\n",
    "- `nn.ModuleList` → 模块列表（可以迭代，不是顺序执行）\n",
    "- `nn.ModuleDict` → 模块字典（key-value 存储子模块）\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 常见层\n",
    "| 层名称 | 主要参数 | 作用 | 使用场景推荐 |\n",
    "|--------|----------|------|--------------|\n",
    "| `nn.Linear(in_features, out_features, bias=True)` | `in_features`: 输入维度<br>`out_features`: 输出维度<br>`bias`: 是否加偏置 | 全连接层（线性变换） | 用于 MLP、分类器最后一层、特征映射 |\n",
    "| `nn.Conv1d(in_c, out_c, kernel_size, stride=1, padding=0)` | `in_c`: 输入通道数<br>`out_c`: 输出通道数<br>`kernel_size`: 卷积核大小<br>`stride`: 步长<br>`padding`: 填充 | 一维卷积 | 常用于文本/时间序列特征提取 |\n",
    "| `nn.Conv2d(in_c, out_c, kernel_size, stride=1, padding=0)` | 同上 | 二维卷积 | 常用于图像特征提取 (CNN) |\n",
    "| `nn.Conv3d(in_c, out_c, kernel_size, stride=1, padding=0)` | 同上 | 三维卷积 | 视频、医学图像 (MRI/CT) |\n",
    "| `nn.ConvTranspose1d(in_c, out_c, kernel_size, stride=1, padding=0)` | 参数同 Conv1d | 反卷积 (转置卷积) | 常用于生成模型、序列解码 |\n",
    "| `nn.ConvTranspose2d(in_c, out_c, kernel_size, stride=1, padding=0)` | 参数同 Conv2d | 二维转置卷积 | 图像上采样 (GAN/Segmentation) |\n",
    "| `nn.ConvTranspose3d(in_c, out_c, kernel_size, stride=1, padding=0)` | 参数同 Conv3d | 三维转置卷积 | 视频/3D 重建 |\n",
    "| `nn.RNN(input_size, hidden_size, num_layers=1, batch_first=True)` | `input_size`: 输入特征维度<br>`hidden_size`: 隐藏层大小<br>`num_layers`: 堆叠层数 | 循环神经网络 | 处理短序列，简单 NLP/时间序列 |\n",
    "| `nn.LSTM(input_size, hidden_size, num_layers=1, batch_first=True)` | 同 RNN，内部有 `cell state` | 长短期记忆网络 | NLP 序列建模，长依赖任务 |\n",
    "| `nn.GRU(input_size, hidden_size, num_layers=1, batch_first=True)` | 同 RNN，简化版 LSTM | 门控循环单元 | NLP，推荐比 RNN 更优先 |\n",
    "| `nn.BatchNorm1d(num_features)` | `num_features`: 通道数 | 一维批归一化 | 序列/表格特征 |\n",
    "| `nn.BatchNorm2d(num_features)` | `num_features`: 通道数 | 二维批归一化 | CNN 图像卷积输出 |\n",
    "| `nn.BatchNorm3d(num_features)` | `num_features`: 通道数 | 三维批归一化 | 视频、3D 图像 |\n",
    "| `nn.LayerNorm(normalized_shape)` | `normalized_shape`: 要归一化的维度 | 层归一化 | NLP Transformer，RNN，稳定训练 |\n",
    "| `nn.GroupNorm(num_groups, num_channels)` | `num_groups`: 分组数<br>`num_channels`: 总通道数 | 分组归一化 | 适合小 batch 的图像任务 |\n",
    "| `nn.InstanceNorm(num_features)` | `num_features`: 通道数 | 实例归一化 | 风格迁移，图像生成 |\n",
    "| `nn.Dropout(p=0.5)` | `p`: 丢弃概率 | 随机丢弃神经元 | 全连接层防过拟合 |\n",
    "| `nn.Dropout2d(p=0.5)` | `p`: 丢弃概率 | 丢弃整张 feature map | CNN 模型防过拟合 |\n",
    "| `nn.Dropout3d(p=0.5)` | `p`: 丢弃概率 | 丢弃 3D 特征块 | 视频/3D CNN |\n",
    "| `nn.Embedding(num_embeddings, embedding_dim)` | `num_embeddings`: 词表大小<br>`embedding_dim`: 向量维度 | 嵌入层（索引 → 向量） | NLP 词向量，ID 特征编码 |\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 激活函数\n",
    "| 激活函数 | 解释 | 使用场景推荐 |\n",
    "|----------|------|--------------|\n",
    "| `nn.ReLU()` | **修正线性单元**：\\(f(x)=\\max(0, x)\\)。计算简单，梯度稳定。输出范围[0,+∞) | 深度神经网络的默认激活函数，CNN、MLP 常用。 |\n",
    "| `nn.Sigmoid()` | **S 型函数**：\\(f(x)=\\frac{1}{1+e^{-x}}\\)，输出范围 (0,1)。 | 二分类任务（输出概率），但易梯度消失，隐藏层少用。 |\n",
    "| `nn.Tanh()` | **双曲正切函数**：\\(f(x)=\\tanh(x)\\)，输出范围 (-1,1)。 | 比 Sigmoid 更对称，早期 RNN 常用，但已逐渐被 ReLU/变体替代。 |\n",
    "| `nn.Softmax(dim)` | 将输入转为概率分布：\\(f(x_i)=\\frac{e^{x_i}}{\\sum_j e^{x_j}}\\)。 | 多分类输出层常用（配合 CrossEntropyLoss 时不要手动加）。 |\n",
    "| `nn.LogSoftmax(dim)` | Softmax 后再取 log，数值更稳定。 | 搭配 `nn.NLLLoss()` 使用。 |\n",
    "| `nn.LeakyReLU()` | ReLU 改进版：负区间保留小斜率（默认 0.01）。 | 解决 ReLU “神经元死亡”问题。 |\n",
    "| `nn.ELU()` | Exponential Linear Unit：负区间使用指数函数平滑过渡。 | 在深层网络中比 ReLU 更鲁棒，但计算比 ReLU 慢。 |\n",
    "| `nn.SELU()` | Scaled ELU，自带缩放因子。 | 适合 **自归一化网络（Self-Normalizing NN）**，需配合 `nn.AlphaDropout`。 |\n",
    "| `nn.GELU()` | Gaussian Error Linear Unit，结合 Sigmoid 和 ReLU 特性，平滑。 | Transformer（BERT、GPT 等）常用，效果优于 ReLU。 |\n",
    "| `nn.SiLU()` | Sigmoid Linear Unit，又称 Swish：\\(x \\cdot \\sigma(x)\\)。 | 在深度学习模型（尤其 CNN）中表现良好，部分替代 ReLU。 |\n",
    "\n",
    "###  🔹 激活函数选择指南\n",
    "\n",
    "A[选择激活函数] --> B{位置在哪里?}\n",
    "\n",
    "B --> C[隐藏层] \\\n",
    "B --> D[输出层]\n",
    "\n",
    "%% 隐藏层\\\n",
    "C --> C1{任务复杂度?}  \\\n",
    "C1 --> C2[一般任务 → ReLU]\\\n",
    "C1 --> C3[深层网络/避免死神经元 → LeakyReLU / ELU]\\\n",
    "C1 --> C4[Transformer / NLP / 高级模型 → GELU / SiLU]\\\n",
    "C1 --> C5[需要自归一化 → SELU (+ AlphaDropout)]\n",
    "\n",
    "%% 输出层\\\n",
    "D --> D1{任务类型?}\\\n",
    "D1 --> D2[二分类 → Sigmoid]\\\n",
    "D1 --> D3[多分类 → Softmax]\\\n",
    "D1 --> D4[需要 log 概率 → LogSoftmax]\\\n",
    "D1 --> D5[回归任务 → 无激活 / Tanh (范围限制)]\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "### 📌 **解释**：\n",
    "- **隐藏层推荐**：  \n",
    "  - 默认 → `ReLU`  \n",
    "  - 想避免“神经元死亡” → `LeakyReLU` / `ELU`  \n",
    "  - NLP / Transformer 系列 → `GELU` / `SiLU`  \n",
    "  - 特殊情况（自归一化网络） → `SELU`  \n",
    "\n",
    "- **输出层推荐**：  \n",
    "  - 二分类 → `Sigmoid`  \n",
    "  - 多分类 → `Softmax`（一般直接用 `nn.CrossEntropyLoss()`，内部会处理）  \n",
    "  - 多分类 + 需要 log 概率 → `LogSoftmax` + `NLLLoss`  \n",
    "  - 回归 → **不加激活函数**（线性输出），如果要限制范围，可以用 `Tanh`  \n",
    "\n",
    "---\n",
    "![激活函数输出示意图](激活函数输出值示意图.png)\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 损失函数\n",
    "| 损失函数 | 解释 |\n",
    "|----------|------|\n",
    "| `nn.MSELoss()` | **均方误差（MSE）**，常用于回归。 |\n",
    "| `nn.L1Loss()` | **平均绝对误差（MAE）**，比 MSE 对异常值更鲁棒|\n",
    "| `nn.CrossEntropyLoss()` | **交叉熵损失**，多分类常用，内部包含 `LogSoftmax + NLLLoss`，输入应为 logits（未过 softmax 的值）。 |\n",
    "| `nn.NLLLoss()` | **负对数似然损失**，输入必须是 log softmax 概率，通常和 `nn.LogSoftmax` 搭配使用。 |\n",
    "| `nn.BCELoss()` | **二分类交叉熵损失**，输入必须是 [0,1] 概率（通常先过 sigmoid）。 |\n",
    "| `nn.BCEWithLogitsLoss()` | **带 Logits 的二分类交叉熵损失**，内部集成 sigmoid，更稳定，推荐替代 `BCELoss`。 |\n",
    "| `nn.HingeEmbeddingLoss()` | **合页嵌入损失**，用于相似度学习，衡量样本对是否相似（+1）或不相似（-1）。 |\n",
    "| `nn.MarginRankingLoss()` | **排序边界损失**，用于排序任务，约束：\\(\\max(0, -y \\cdot (x1 - x2) + margin)\\)。 |\n",
    "| `nn.TripletMarginLoss()` | **三元组损失**，常用于人脸识别/度量学习，约束：\\( d(anchor, positive) + margin < d(anchor, negative)\\)。 |\n",
    "| `nn.KLDivLoss()` | **KL 散度损失**，度量两个概率分布差异，常用于知识蒸馏、概率建模。 |\n",
    "\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 池化\n",
    "- `nn.MaxPool1d/2d/3d`\n",
    "- `nn.AvgPool1d/2d/3d`\n",
    "- `nn.AdaptiveAvgPool2d`\n",
    "- `nn.AdaptiveMaxPool2d`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bcffa3c8-56fb-48fc-bfce-e9faab7ec00d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0e58f71c-ae45-4073-a2ab-57fd5887230f",
   "metadata": {},
   "source": [
    "# 1. 容器（Containers）"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0d5e4122-7789-409c-989a-c22535cd69a4",
   "metadata": {},
   "source": [
    "## 🔹 1. nn.Module 概念\n",
    "\n",
    "nn.Module = 神经网络组件的基类。\n",
    "\n",
    "可以表示 单个层（如线性层、卷积层），也可以表示 整个网络（由很多子层组合）。\n",
    "\n",
    "所有 torch.nn 的层和损失函数，都是 nn.Module 的子类。\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 2. nn.Module 的核心方法 & 属性\n",
    "| 方法/属性                          | 作用                          |\n",
    "| ------------------------------ | --------------------------- |\n",
    "| `__init__()`                   | 构造函数，用于定义层和参数               |\n",
    "| `forward(input)`               | 前向传播逻辑，定义输入如何计算输出           |\n",
    "| `.parameters()`                | 返回所有需要梯度的参数（`nn.Parameter`） |\n",
    "| `.named_parameters()`          | 带名字的参数迭代器                   |\n",
    "| `.children()`                  | 迭代直接子模块                     |\n",
    "| `.modules()`                   | 递归迭代所有子模块（包含自身）             |\n",
    "| `.state_dict()`                | 返回模型参数字典（用于保存/加载）           |\n",
    "| `.load_state_dict(state_dict)` | 加载参数字典                      |\n",
    "| `.to(device)`                  | 把模型转移到 CPU/GPU              |\n",
    "| `.train()`                     | 切换到训练模式（启用 Dropout/BN）      |\n",
    "| `.eval()`                      | 切换到评估模式（关闭 Dropout/BN）      |\n",
    "\n",
    "### ⚠️ 注意：\n",
    "\n",
    "必须实现 __init__() 和 forward()\n",
    "\n",
    "不要手动调用 forward()，而是用 model(input)，内部会自动调用 forward\n",
    "\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 3. nn.Module 工作流程\n",
    "\n",
    "1、定义层（在 __init__ 中）：把需要的层定义成成员变量（如 self.fc1 = nn.Linear(...)）\n",
    "\n",
    "2、定义计算逻辑（在 forward 中）：输入如何流过这些层，返回输出\n",
    "\n",
    "3、实例化模型并调用：\\\n",
    "model = MyNet() \\\n",
    "output = model(x)（内部会自动调用 forward）\n",
    "\n",
    "---\n",
    "\n",
    "## 🔹 4. 示例代码"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5d594d5-8d97-41d1-a4d5-83b19ed64aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "\n",
    "# 自定义网络\n",
    "class MyNet(nn.Module):\n",
    "    # 构造各个层\n",
    "    def __init__(self):\n",
    "        super(MyNet, self).__init__()  # 继承 nn.Module\n",
    "        self.fc1 = nn.Linear(10, 20)   # 第一层，全连接\n",
    "        self.fc2 = nn.Linear(20, 1)    # 第二层，全连接\n",
    "\n",
    "    # 计算过程\n",
    "    def forward(self, x):\n",
    "        x = F.relu(self.fc1(x))   # 经过第一层 + ReLU\n",
    "        x = torch.sigmoid(self.fc2(x)) # 经过第二层 + Sigmoid\n",
    "        return x\n",
    "\n",
    "# 使用模型\n",
    "model = MyNet()\n",
    "x = torch.randn(5, 10)    # batch_size=5, 输入特征=10\n",
    "output = model(x)\n",
    "print(output.shape)       # [5, 1]\n",
    "\n",
    "# 查看参数\n",
    "for name, param in model.named_parameters():\n",
    "    print(name, param.shape)\n",
    "\n",
    "\"\"\"\n",
    "输出：\n",
    "fc1.weight torch.Size([20, 10])\n",
    "fc1.bias   torch.Size([20])\n",
    "fc2.weight torch.Size([1, 20])\n",
    "fc2.bias   torch.Size([1])\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5dd72d1-125f-4432-92d1-01bf68d1f271",
   "metadata": {},
   "source": [
    "## 🔹 5. 常见坑\n",
    "\n",
    "1、不要手动调用 forward\n",
    "\n",
    "- `y = model(x)   # 正确` \n",
    "- `y = model.forward(x)  # ❌ 不推荐  → 直接调用 forward 会绕过 hooks 等机制。`\n",
    "\n",
    "2、parameters() 只返回需要梯度的参数,如果你有一些 buffer（比如统计量），可以用 register_buffer。\n",
    "\n",
    "3、训练 / 评估模式要切换\n",
    "\n",
    "model.train()  # 启用 dropout, BN \\\n",
    "model.eval()   # 关闭 dropout, BN\n",
    "\n",
    "\n",
    "4、保存/加载参数要用 state_dict()\n",
    "\n",
    "torch.save(model.state_dict(), \"model.pth\")\\\n",
    "model.load_state_dict(torch.load(\"model.pth\"))\n",
    "\n",
    "---\n",
    "\n",
    "## ✅ 总结：\n",
    "nn.Module 就像 乐高积木的基类 —— 你定义的每个积木（层/网络）都要继承它，把层在 __init__ 里搭好，把计算过程写在 forward 里，然后 PyTorch 会帮你自动处理参数管理、梯度计算、保存加载等。"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab23451c-4cef-4acc-b7c4-06e968982898",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c50de1ca-db5f-4c7a-86c4-45c274d9a26e",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0e4e63a-3772-439e-af9b-40dbacbf96ff",
   "metadata": {},
   "source": [
    "# 🔹完整示例：MNIST 分类"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9df85b09-35c6-4770-b81e-bf69d094d01b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torchvision import datasets, transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 1. 数据预处理\n",
    "transform = transforms.Compose([\n",
    "    transforms.ToTensor(),                # 转换为 Tensor\n",
    "    transforms.Normalize((0.5,), (0.5,))  # 标准化到 [-1,1]\n",
    "])\n",
    "\n",
    "\n",
    "train_dataset = datasets.MNIST(root='./data', train=True, download=True, transform=transform)\n",
    "test_dataset  = datasets.MNIST(root='./data', train=False, download=True, transform=transform)\n",
    "\n",
    "# 用 DataLoader，它负责高效迭代已划分好的数据，不涉及划分逻辑，为数据集提供批量加载和迭代的功能\n",
    "train_loader = DataLoader(train_dataset, batch_size=64, shuffle=True)\n",
    "test_loader  = DataLoader(test_dataset, batch_size=1000, shuffle=False)\n",
    "\n",
    "\n",
    "# 2. 定义网络结构\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(Net, self).__init__()\n",
    "        self.fc1 = nn.Linear(28*28, 256)   # 输入层 → 隐藏层1\n",
    "        self.fc2 = nn.Linear(256, 128)     # 隐藏层1 → 隐藏层2\n",
    "        self.fc3 = nn.Linear(128, 10)      # 隐藏层2 → 输出层\n",
    "\n",
    "        self.relu = nn.ReLU()              # 激活函数\n",
    "        self.dropout = nn.Dropout(0.5)     # Dropout 防止过拟合\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(-1, 28*28)              # 展平\n",
    "        x = self.relu(self.fc1(x))         # 全连接 + ReLU\n",
    "        x = self.dropout(x)                # Dropout\n",
    "        x = self.relu(self.fc2(x))         # 全连接 + ReLU\n",
    "        x = self.fc3(x)                    # 输出层（未做Softmax）\n",
    "        return x                           # 交叉熵内部会做LogSoftmax\n",
    "\n",
    "\n",
    "# 3. 初始化模型、损失函数、优化器\n",
    "model = Net()\n",
    "criterion = nn.CrossEntropyLoss()          # 分类任务常用\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "\n",
    "# 4. 训练函数\n",
    "def train(model, loader, optimizer, criterion, device=\"cpu\"):\n",
    "    model.train()\n",
    "    total_loss, correct = 0, 0\n",
    "    for data, target in loader:\n",
    "        data, target = data.to(device), target.to(device)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        output = model(data)\n",
    "\n",
    "        loss = criterion(output, target)   # 计算损失\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        total_loss += loss.item()\n",
    "        pred = output.argmax(dim=1)        # 预测类别\n",
    "        correct += (pred == target).sum().item()\n",
    "\n",
    "    acc = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader), acc\n",
    "\n",
    "\n",
    "# 5. 测试函数\n",
    "def evaluate(model, loader, criterion, device=\"cpu\"):\n",
    "    model.eval()\n",
    "    total_loss, correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for data, target in loader:\n",
    "            data, target = data.to(device), target.to(device)\n",
    "            output = model(data)\n",
    "\n",
    "            loss = criterion(output, target)\n",
    "            total_loss += loss.item()\n",
    "            pred = output.argmax(dim=1)\n",
    "            correct += (pred == target).sum().item()\n",
    "\n",
    "    acc = correct / len(loader.dataset)\n",
    "    return total_loss / len(loader), acc\n",
    "\n",
    "\n",
    "# 6. 训练 + 测试循环\n",
    "for epoch in range(1, 6):  # 训练 5 轮\n",
    "    train_loss, train_acc = train(model, train_loader, optimizer, criterion)\n",
    "    test_loss, test_acc = evaluate(model, test_loader, criterion)\n",
    "    print(f\"Epoch {epoch}: Train Loss={train_loss:.4f}, Train Acc={train_acc:.4f}, \"\n",
    "          f\"Test Loss={test_loss:.4f}, Test Acc={test_acc:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "53eb7d95-958d-4495-88f4-cda1845664a1",
   "metadata": {},
   "source": [
    "### 🔎 涵盖内容\n",
    "\n",
    "神经网络层：nn.Linear\n",
    "\n",
    "激活函数：nn.ReLU\n",
    "\n",
    "正则化：nn.Dropout\n",
    "\n",
    "损失函数：nn.CrossEntropyLoss（内部包含 LogSoftmax + NLLLoss）\n",
    "\n",
    "评估指标：准确率（Accuracy）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "89c4b3dd-badf-467b-9672-ef0cc07e82ef",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:deepseek_env]",
   "language": "python",
   "name": "conda-env-deepseek_env-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
